{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    " \n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def get_parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def get_grads(self, input_,  next_grad):\n",
    "        return []\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.w = np.random.uniform(-1./output_size**0.5, 1./output_size**0.5, size=(output_size, input_size))\n",
    "        self.b = np.zeros(output_size)\n",
    "\n",
    "    def output(self, x):\n",
    "        return np.dot(x, self.w.T) + self.b\n",
    "    \n",
    "    def grad(self, input_, grad_next_layer):\n",
    "        return np.dot(grad_next_layer, self.w)\n",
    "    \n",
    "    def update_parameters(self, input_, learning_rate, grad_next_layer):\n",
    "        self.w -= learning_rate * np.dot(input_.T, grad_next_layer)\n",
    "        self.b -= learning_rate * grad_next_layer\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "class SigmoidLayer(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1. / (1. + np.exp(x))\n",
    "    \n",
    "    def output(self, x):\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def grad(self, input_, grad_next_layer):\n",
    "        s = self.sigmoid(input_)\n",
    "        return s * (1 - s)\n",
    "        \n",
    "class SoftmaxLayer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def output(self, x):\n",
    "        probs = np.exp(x)\n",
    "        return probs / np.sum(probs, axis=1)[:,None]\n",
    "    \n",
    "    def grad(self, input_, grad_next_layer):\n",
    "        v = input_ - input_**2\n",
    "        return v\n",
    "  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def train(self, n_epochs, batch_size, train, valid):\n",
    "        train_x = train[0]\n",
    "        train_y = train[1]\n",
    "        n_batch = int(train_x.shape[0] / batch_size)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            for current_batch in range(n_batch):\n",
    "                fprop_results = self.fprop(train_x[current_batch*batch_size:(current_batch+1)*batch_size])\n",
    "                grads = self.bprop(train_x[current_batch*batch_size:(current_batch+1)*batch_size],\n",
    "                                   train_y[current_batch*batch_size:(current_batch+1)*batch_size],\n",
    "                                  fprop_results)\n",
    "                    \n",
    "    def fprop(self, data_x, data_y):\n",
    "        fprop_results = [data_x]\n",
    "        # fprop\n",
    "        for layer in self.layers:\n",
    "            fprop_results.append(layer.output(fprop_results[-1]))\n",
    "            \n",
    "        cost = -np.log(fprop_results[-1][np.arange(fprop_results[-1].shape[0]), data_y]).mean()\n",
    "        return cost, fprop_results\n",
    "    \n",
    "    def bprop(self, data_x, data_y, fprop_results):\n",
    "        activations = fprop_results[-1]\n",
    "        dc = -1 / activations * (np.arange(activations.shape[1])[:, None] == data_y).T\n",
    "\n",
    "        grads = [dc]\n",
    "        for index, layer in enumerate(self.layers[::-1]):\n",
    "            print index, grads[-1].shape\n",
    "            grads.append(layer.grad(fprop_results[len(fprop_results)-index-1], grads[-1]))\n",
    "            \n",
    "        return grads\n",
    "\n",
    "    def verify_gradients(self, input_size, n_classes):\n",
    "    \n",
    "        random_x = np.random.uniform(0, 1, size=(1, input_size))\n",
    "        random_y = np.random.randint(10)\n",
    "        epsilon = 10**-5\n",
    "        \n",
    "        cost, fprop_results = self.fprop(random_x, random_y)\n",
    "        grads = self.bprop(random_x, random_y, fprop_results)\n",
    "        \n",
    "        # compute gradients with finite difference\n",
    "        for index, layer in enumerate(layers):\n",
    "            for parameter in zip(layer.get_parameters(), layer.get_grads(fprop_results[index], grads[-index])):\n",
    "                \n",
    "                if len(parameter.shape) == 2:\n",
    "                    numerical_estimate = np.zeros(parameter.shape)\n",
    "                    for i in range(parameter.shape[0]):\n",
    "                        for j in range(parameter.shape[1]):\n",
    "                            parameter[i, j] += epsilon\n",
    "                            cost_right, discard = self.fprop(random_x, random_y)\n",
    "                            parameter[i, j] -= 2 * epsilon\n",
    "                            cost_left, discard = self.fprop(random_x, random_y)\n",
    "                            numerical_estimate[i, j] = (cost_right - cost_left) / (2 * epsilon)\n",
    "                            \n",
    "                    print np.allclose(numerical_estimate, numerical_estimate)\n",
    "    \n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 10)\n",
      "1 (1, 10)\n",
      "2 (1, 100)\n",
      "3 (1, 100)\n"
     ]
    }
   ],
   "source": [
    "# build actual mlp\n",
    "\n",
    "layers = [LinearLayer(784, 100), SigmoidLayer(), LinearLayer(100, 10), SoftmaxLayer()]\n",
    "mlp = MLP(layers)\n",
    "\n",
    "mlp.verify_gradients(784, 10)\n",
    "# mlp.train(5, 100, train_set, valid_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IFT6266 (Python 2.7)",
   "language": "python",
   "name": "ift6266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
