{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    " \n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def get_parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def get_grads(self, input_,  next_grad):\n",
    "        return []\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.w = np.random.uniform(-1./output_size**0.5, 1./output_size**0.5, size=(output_size, input_size))\n",
    "        self.b = np.zeros(output_size)\n",
    "\n",
    "    def output(self, x):\n",
    "        return np.dot(x, self.w.T) + self.b\n",
    "    \n",
    "    def grad(self, input_, grad_next_layer):\n",
    "        return np.dot(grad_next_layer, self.w)\n",
    "    \n",
    "    def update_parameters(self, input_, learning_rate, grad_next_layer):\n",
    "        self.w -= learning_rate * np.dot(input_.T, grad_next_layer)\n",
    "        self.b -= learning_rate * grad_next_layer\n",
    "        \n",
    "    def get_grads(self, input_, grad_next_layer):\n",
    "        return [np.dot(grad_next_layer.T, input_), grad_next_layer]\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "class SigmoidLayer(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1. / (1. + np.exp(x))\n",
    "    \n",
    "    def output(self, x):\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def grad(self, input_, grad_next_layer):\n",
    "        s = self.sigmoid(input_)\n",
    "        # print 's', grad_next_layer.shape, s.shape\n",
    "        return grad_next_layer * s * (1 - s)\n",
    "        \n",
    "class SoftmaxLayer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def output(self, x):\n",
    "        x_max = np.max(x, axis=1)\n",
    "        probs = np.exp(x-x_max)\n",
    "        return probs / np.sum(probs, axis=1)\n",
    "    \n",
    "    def grad(self, input_, grad_next_layer):\n",
    "        sam = self.output(input_)\n",
    "        return sam * (np.diagflat(np.ones(input_.shape[-1])) - np.array([np.roll(sam[0,:], i) for i in range(input_.shape[-1])])).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11620733 -0.01987407 -0.03567919 -0.03516075 -0.02549333]]\n",
      "[ 0.11620733 -0.01987407 -0.03567919 -0.03516075 -0.02549333]\n",
      "[[-0.01987407  0.12614338 -0.03935923 -0.03878731 -0.02812277]]\n",
      "[-0.02812277  0.12819324 -0.02192393 -0.03935923 -0.03878731]\n",
      "[[-0.03567919 -0.03935923  0.19515966 -0.06963345 -0.05048779]]\n",
      "[-0.06963345 -0.05048779  0.23014067 -0.03935923 -0.0706602 ]\n",
      "[[-0.03516075 -0.03878731 -0.06963345  0.19333567 -0.04975416]]\n",
      "[-0.06963345 -0.06862162 -0.04975416  0.22679655 -0.03878731]\n",
      "[[-0.02549333 -0.02812277 -0.05048779 -0.04975416  0.15385804]]\n",
      "[-0.02812277 -0.05048779 -0.04975416 -0.03607429  0.16443901]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 10**-5\n",
    "\n",
    "s = SoftmaxLayer()\n",
    "x = np.random.uniform(0, 1, size=(1, 5))\n",
    "grad = s.grad(x, 1)\n",
    "\n",
    "for i in range(5):\n",
    "    x[0, i] += epsilon\n",
    "    plus = s.output(x)\n",
    "    x[0, i] -= 2*epsilon\n",
    "    moins = s.output(x)\n",
    "    x[0, i] += epsilon\n",
    "    \n",
    "    print (plus - moins) / (2 * epsilon)\n",
    "    print grad[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def train(self, n_epochs, batch_size, train, valid):\n",
    "        train_x = train[0]\n",
    "        train_y = train[1]\n",
    "        n_batch = int(train_x.shape[0] / batch_size)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            for current_batch in range(n_batch):\n",
    "                fprop_results = self.fprop(train_x[current_batch*batch_size:(current_batch+1)*batch_size])\n",
    "                grads = self.bprop(train_x[current_batch*batch_size:(current_batch+1)*batch_size],\n",
    "                                   train_y[current_batch*batch_size:(current_batch+1)*batch_size],\n",
    "                                  fprop_results)\n",
    "                    \n",
    "    def fprop(self, data_x, data_y):\n",
    "        fprop_results = [data_x]\n",
    "        # fprop\n",
    "        for layer in self.layers:\n",
    "            fprop_results.append(layer.output(fprop_results[-1]))\n",
    "            \n",
    "        cost = -np.log(fprop_results[-1][np.arange(fprop_results[-1].shape[0]), data_y]).mean()\n",
    "        return cost, fprop_results\n",
    "    \n",
    "    def bprop(self, data_x, data_y, fprop_results):\n",
    "        activations = fprop_results[-1]\n",
    "        dc = -1 / activations * (np.arange(activations.shape[1])[:, None] == data_y).T\n",
    "\n",
    "        grads = [dc]\n",
    "        for index, layer in enumerate(self.layers[::-1]):\n",
    "            grads.append(layer.grad(fprop_results[-index-1], grads[-1]))\n",
    "            \n",
    "        return grads\n",
    "\n",
    "    def verify_gradients(self, input_size, n_classes):\n",
    "    \n",
    "        random_x = np.random.uniform(0, 1, size=(1, input_size))\n",
    "        random_y = np.random.randint(10)\n",
    "        epsilon = 10**-5\n",
    "        \n",
    "        cost, fprop_results = self.fprop(random_x, random_y)\n",
    "        grads = self.bprop(random_x, random_y, fprop_results)\n",
    "        \n",
    "        for index in range(len(grads)):\n",
    "            print index, fprop_results[index].shape, grads[-index-1].shape\n",
    "        \n",
    "        # compute gradients with finite difference\n",
    "        for index, layer in enumerate(layers):\n",
    "            for parameter, grad in zip(layer.get_parameters(), layer.get_grads(fprop_results[index], grads[-index-2])):\n",
    "                \n",
    "                numerical_estimate = np.zeros(parameter.shape)\n",
    "                \n",
    "                # in case the parameter is a matrix\n",
    "                if len(parameter.shape) == 2:\n",
    "                    for i in range(parameter.shape[0]):\n",
    "                        for j in range(parameter.shape[1]):\n",
    "                            parameter[i, j] += epsilon\n",
    "                            cost_right, discard = self.fprop(random_x, random_y)\n",
    "                            parameter[i, j] -= 2 * epsilon\n",
    "                            cost_left, discard = self.fprop(random_x, random_y)\n",
    "                            parameter[i, j] += epsilon\n",
    "                            numerical_estimate[i, j] = (cost_right - cost_left) / (2 * epsilon)\n",
    "                            \n",
    "                # in case the parameter is a vector\n",
    "                if len(parameter.shape) == 1:\n",
    "                    numerical_estimate = np.zeros(parameter.shape)\n",
    "                    for i in range(parameter.shape[0]):\n",
    "                        parameter[i] += epsilon\n",
    "                        cost_right, discard = self.fprop(random_x, random_y)\n",
    "                        parameter[i] -= 2 * epsilon\n",
    "                        cost_left, discard = self.fprop(random_x, random_y)\n",
    "                        parameter[i] += epsilon\n",
    "                        numerical_estimate[i] = (cost_right - cost_left) / (2 * epsilon)\n",
    "                            \n",
    "                print np.allclose(numerical_estimate, grad)\n",
    "                print np.abs(numerical_estimate- grad).mean()\n",
    "    \n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 100) (10, 100)\n",
      "1 (1, 100) (10, 100)\n",
      "2 (1, 100) (10, 100)\n",
      "3 (1, 10) (10, 10)\n",
      "4 (1, 10) (1, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,10) and (1,100) not aligned: 10 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-384-fff020c1ae16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverify_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# mlp.train(5, 100, train_set, valid_set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-229-98858f3d873f>\u001b[0m in \u001b[0;36mverify_gradients\u001b[1;34m(self, input_size, n_classes)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# compute gradients with finite difference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfprop_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mnumerical_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-381-4bdec0830161>\u001b[0m in \u001b[0;36mget_grads\u001b[1;34m(self, input_, grad_next_layer)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_next_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_next_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_next_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (100,10) and (1,100) not aligned: 10 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# build actual mlp\n",
    "\n",
    "layers = [LinearLayer(100, 100), SigmoidLayer(), LinearLayer(100, 10), SoftmaxLayer()]\n",
    "mlp = MLP(layers)\n",
    "\n",
    "mlp.verify_gradients(100, 10)\n",
    "# mlp.train(5, 100, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [1, 2, 3, 4],\n",
       "       [1, 2, 3, 4],\n",
       "       [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3, 4]])\n",
    "b = a.repeat(4, axis=0)\n",
    "\n",
    "b[range(b.shape[0]), range(b.shape[0])] -= np.array(range(4))[None,:]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "print np.roll(a, 2)\n",
    "print a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IFT6266 (Python 2.7)",
   "language": "python",
   "name": "ift6266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
