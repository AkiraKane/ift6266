{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "\n",
    "We begin by importing the NumPy package. It is common practice to rename NumPy's namespace to the shorthand `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays\n",
    "\n",
    "The most important object in NumPy is an array, which is an N-dimensional tensor. Each array has a shape (its dimensions) and a data-type (`dtype`, but casting happens automatically as it does in Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), dtype('int64'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([1, 2, 3]) # A vector\n",
    "W.shape, W.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  1. ,  1.5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = W / 2\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.arange(9).reshape((3, 3))\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "Most algebraic manipulations are applied element-wise and not inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  4],\n",
       "       [ 9, 16, 25],\n",
       "       [36, 49, 64]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W * W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "NumPy performs broadcasting, which means that arrays with fewer dimensions are automatically resized to be compatible with the other array. This is useful for example when adding biases to a minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [0, 4, 5],\n",
       "       [0, 7, 8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.array([0, 1, 1])\n",
    "W * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3), (3,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebra\n",
    "\n",
    "The most important operations is probably the matrix-matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12, 21])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.array([1, 1, 1])\n",
    "np.dot(W, V)  # A matrix-vector product, sum over columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15,  18,  21],\n",
       "       [ 42,  54,  66],\n",
       "       [ 69,  90, 111]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(W, W)  # Matrix-matrix product of W with itself i.e. W squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15,  18,  21],\n",
       "       [ 42,  54,  66],\n",
       "       [ 69,  90, 111]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dot(W)  # Alternative notation, equal to np.dot(W, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  14,  23],\n",
       "       [ 14,  50,  86],\n",
       "       [ 23,  86, 149]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dot(W.T)  # W.T takes the product of W, alternatively use W.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When coding neural networks, a useful programming abstraction is to create a class for each layer which implements two functions: The forward propagation, and the backward propagation. In Python, classes are created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 3 allows for the shorter `class Layer:` notation\n",
    "class Layer(object):\n",
    "    # A method is defined using `def`\n",
    "    def forward_propagation(self, input_):\n",
    "        # The variable `self` is the instance of this class\n",
    "        # This returns the transformed input\n",
    "        pass\n",
    "    \n",
    "    def backward_propagation(self, gradient_wrt_output):\n",
    "        # This takes the gradient w.r.t. the output,\n",
    "        # and returns the gradient w.r.t. the input\n",
    "        pass\n",
    "    \n",
    "    def update_parameters(self, input_, gradient_wrt_output, learning_rate):\n",
    "        # This takes the input, the gradient w.r.t. the output and\n",
    "        # updates the parameters using the given learning rate\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a linear transformation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class inherits from Layer\n",
    "class Linear(Layer):\n",
    "    # __init__ is the constructor method of this class\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.W = np.random.randn(input_dim, output_dim)\n",
    "        \n",
    "    def forward_propagation(self, input_):\n",
    "        return input_.dot(self.W)\n",
    "    \n",
    "    def backward_propagation(self, gradient_wrt_output):\n",
    "        return gradient_wrt_output.dot(self.W.T)\n",
    "    \n",
    "    def update_parameters(self, input_, gradient_wrt_output, learning_rate):\n",
    "        self.W -= learning_rate * input_.T.dot(gradient_wrt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extend this implementation to account for biases, and implement layers for the sigmoid and softmax operations as well. They can then be strung together to form an MLP.\n",
    "\n",
    "*Note: This is largely pseudo-code, and untested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.ones(10)\n",
    "A[None, :, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f16dee669d30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define the network as a series of layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Forward propagate the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the network as a series of layers\n",
    "layers = [Linear(784, 100), Sigmoid(), Linear(100, 10), Softmax()]\n",
    "\n",
    "# Forward propagate the data\n",
    "inputs = [get_minibatch()]\n",
    "for layer in layers:\n",
    "    inputs.append(layer.forward_propagation(inputs[-1]))\n",
    "\n",
    "# Calculate the cost\n",
    "activations = inputs[-1]\n",
    "c = -np.log(activations[np.arange(activations.shape[0], targets)]).mean()\n",
    "\n",
    "# Get the gradient of the cost with respect to the softmax output\n",
    "# The gradient of the logarithm is 1 / x, the non-indexed entries are zero, hence\n",
    "dc = -1 / activations * (np.arange(activations.shape[0])[:, None] == targets).T\n",
    "\n",
    "# Now go layer by layer in reverse and perform the backward propagation\n",
    "grads_wrt_inputs = [dc]\n",
    "for layer in layers[::-1]:\n",
    "    layer.update_parameters(inputs[layers.index(layer)], grad_wrt_inputs[-1], 0.001)\n",
    "    grads_wrt_inputs.append(layer.backward_propagation(grads_wrt_inputs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To calculate the gradient analytically, loop over each individual parameter, set it to $\\theta - \\delta$, calculate the cost, set it to $\\theta + \\delta$, calculate the cost again (then set it to its original value). The gradient for $\\theta$ is then the difference of the two costs divided by $2\\delta$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
