{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN Black (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "# Let's load and process the dataset\n",
    "from fuel.datasets.dogs_vs_cats import DogsVsCats\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.server import start_server\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers.image import RandomFixedSizeCrop, MinimumImageDimensions\n",
    "from fuel.transformers import Flatten, ScaleAndShift, Cast\n",
    "import numpy\n",
    "import socket\n",
    "import sys\n",
    "from transformers import FixedSizeCrops, DownscaleMinDimension\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "# Load the training set\n",
    "train = DogsVsCats(('test',))\n",
    "# We now create a \"stream\" over the dataset which will return shuffled batches\n",
    "# of size 128. Using the DataStream.default_stream constructor will turn our\n",
    "# 8-bit images into floating-point decimals in [0, 1].\n",
    "stream = DataStream.default_stream(\n",
    "    train,\n",
    "    iteration_scheme=SequentialScheme(train.num_examples, batch_size)\n",
    ")\n",
    "\n",
    "upscaled_stream = MinimumImageDimensions(stream, (100, 100), which_sources=('image_features',))\n",
    "downscaled_stream = DownscaleMinDimension(upscaled_stream, 100, which_sources=('image_features',))\n",
    "\n",
    "# Our images are of different sizes, so we'll use a Fuel transformer\n",
    "# to take random crops of size (32 x 32) from each image\n",
    "cropped_stream = FixedSizeCrops(\n",
    "    downscaled_stream, (100, 100), which_sources=('image_features',))\n",
    "\n",
    "# We'll use a simple MLP, so we need to flatten the images\n",
    "# from (channel, width, height) to simply (features,)\n",
    "float_stream = ScaleAndShift(cropped_stream, 1./255, 0, which_sources=('image_features',))\n",
    "float32_stream = Cast(float_stream, numpy.float32, which_sources=('image_features',))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006\n",
      "To override, pass 'load_from_config=False' to Session\n"
     ]
    }
   ],
   "source": [
    "from blocks.extensions.saveload import load\n",
    "\n",
    "with open('/tmp/train_bn2', 'rb') as src:\n",
    "    main_loop_loaded = load(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = main_loop_loaded.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3, 100, 200)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ei = float32_stream.get_epoch_iterator()\n",
    "d = next(ei)\n",
    "\n",
    "print d[0].shape\n",
    "print len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor\n",
    "\n",
    "X = tensor.tensor4('image_features', dtype='float32')\n",
    "f = theano.function(model.inputs, model.outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  9.99719560e-01],\n",
       "        [  9.99987602e-01],\n",
       "        [  9.99820054e-01],\n",
       "        [  9.99993920e-01],\n",
       "        [  8.32583464e-04],\n",
       "        [  7.41220079e-04],\n",
       "        [  2.30501202e-04],\n",
       "        [  2.69188226e-04],\n",
       "        [  1.15784153e-03],\n",
       "        [  1.15517469e-04],\n",
       "        [  5.27473057e-05],\n",
       "        [  9.99813616e-01],\n",
       "        [  2.82882102e-04],\n",
       "        [  2.68404779e-04],\n",
       "        [  6.03929511e-04],\n",
       "        [  8.00209679e-03],\n",
       "        [  9.99966383e-01],\n",
       "        [  9.99957800e-01],\n",
       "        [  1.26400764e-05],\n",
       "        [  1.13869646e-04],\n",
       "        [  9.99950528e-01],\n",
       "        [  1.14523026e-03],\n",
       "        [  9.99976993e-01],\n",
       "        [  9.78722751e-01],\n",
       "        [  1.67087637e-04]], dtype=float32)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.config.exception_verbosity = 'low'\n",
    "\n",
    "f(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_loop_loaded.algorithm.step_rule.learning_rate.set_value(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN RESUMED\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 119\n",
      "\t iterations_done: 71380\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: 9cdd96d3a60f42e5ac24da2da7526f50\n",
      "\t training_started: True\n",
      "Log records from the iteration 71380:\n",
      "\t error: 0.0347334221005\n",
      "\t loss: 0.0833946987987\n",
      "\t time_read_data_this_epoch: 0.376342535019\n",
      "\t time_read_data_total: 45.5398755074\n",
      "\t time_train_this_epoch: 453.025973797\n",
      "\t time_train_total: 53913.5800719\n",
      "\t training_finished: True\n",
      "\t valid_error: 0.0489999689162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_loop_loaded.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ei = float32_stream.get_epoch_iterator()\n",
    "prediction = []\n",
    "\n",
    "for d in ei:\n",
    "    prediction.append(f(d[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "print len(prediction)*prediction[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_csv = open('output.csv','w')\n",
    "out_csv.write('id,label\\n')\n",
    "i=1\n",
    "for p_batch in prediction:\n",
    "    for p in p_batch:\n",
    "        out_csv.write('%d,%d\\n' % (i, (p>0.5)*1))\n",
    "        i += 1\n",
    "                      \n",
    "out_csv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
